# SystÃ¨me de DÃ©duplication et Compression de Fichiers

### Autheur: Julien HUYNH, Samuel LUNION

## ğŸ“‹ Vue d'ensemble

SystÃ¨me avancÃ© de dÃ©duplication de fichiers avec dÃ©coupage intelligent, compression et stockage optimisÃ©.

```
[Fichiers] â†’ [Chunking Adaptatif] â†’ [Compression] â†’ [DÃ©duplication] â†’ [Stockage OptimisÃ©]
```

## ğŸ—ï¸ Architecture

### Base de donnÃ©es

#### Table `chunks`

```sql
CREATE TABLE chunks (
    chunk_hash VARCHAR(64) PRIMARY KEY,  -- Hash unique du chunk
    file_path TEXT NOT NULL,             -- Chemin de stockage
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Table `file_chunks`

```sql
CREATE TABLE file_chunks (
    id SERIAL PRIMARY KEY,
    filename TEXT NOT NULL,              -- Nom du fichier original
    chunk_hash VARCHAR(64),              -- RÃ©fÃ©rence au chunk
    chunk_number INT NOT NULL,           -- Position dans le fichier
    UNIQUE(filename, chunk_number)
);
```

### Structure de stockage

```
/storage/
â”œâ”€â”€ [hash1]  -- Contenu compressÃ© du chunk 1 avec mÃ©tadonnÃ©es
â”œâ”€â”€ [hash2]  -- Contenu compressÃ© du chunk 2 avec mÃ©tadonnÃ©es
â””â”€â”€ [hash3]  -- Contenu compressÃ© du chunk 3 avec mÃ©tadonnÃ©es
```

## ğŸ”„ Processus

### 1. DÃ©coupage Adaptatif (Content-Defined Chunking)

Le systÃ¨me utilise l'algorithme de Rabin pour un dÃ©coupage intelligent adaptÃ© Ã  la taille du fichier :

- < 10 KB: chunks de ~1 KB (512-2048 bytes)
- < 100 KB: chunks de ~4 KB (1024-4096 bytes)
- < 1 MB: chunks de ~8 KB (2048-8192 bytes)
- < 10 MB: chunks de ~16 KB (4096-16384 bytes)
- < 100 MB: chunks de ~32 KB (8192-32768 bytes)
- > 100 MB: chunks de ~64 KB (16384-65536 bytes)

### 2. Compression

Utilise la bibliothÃ¨que Zstd avec :
- Niveau de compression : 22 (maximum)
- Format des chunks compressÃ©s : [taille originale (8 bytes)][donnÃ©es compressÃ©es]
- DÃ©tection automatique des fichiers prÃ©-compressÃ©s (jpg, png, zip, etc.)

### 3. DÃ©duplication et Stockage

1. Calcul du hash SHA-256 pour chaque chunk
2. VÃ©rification des doublons dans la base de donnÃ©es
3. Stockage unique des chunks compressÃ©s
4. Maintien des rÃ©fÃ©rences et mÃ©tadonnÃ©es

### 4. Reconstruction

1. RequÃªte des chunks par nom de fichier
2. DÃ©compression des chunks
3. Assemblage dans l'ordre original
4. Sauvegarde du fichier reconstruit

## ğŸ“Š Statistiques et Monitoring

Le systÃ¨me fournit des analyses dÃ©taillÃ©es :

### DÃ©duplication
- Nombre de chunks uniques
- Nombre total de rÃ©fÃ©rences
- Taux de dÃ©duplication
- Espace Ã©conomisÃ©

### Compression
- Taille originale vs compressÃ©e par chunk
- Ratio de compression moyen
- Performance par type de fichier
- Temps d'exÃ©cution

## ğŸ”¬ Benchmarking

Le systÃ¨me inclut des outils de benchmark pour comparer :
- Compression globale vs par chunk
- Temps d'exÃ©cution
- Ratios de compression
- Recommandations adaptÃ©es au type de fichier

## ğŸ’¡ Exemple d'utilisation

```java
// Initialisation avec compression
CompressedChunkStorageSystem storageSystem = new CompressedChunkStorageSystem();
FileReconstructor reconstructor = new FileReconstructor(storageSystem);

// Traitement d'un dossier
processFolder(storageSystem, "data-files");

// Affichage des statistiques
storageSystem.printChunkDetails();
storageSystem.printCompressionStats();

// Reconstruction
reconstructor.reconstructFile("fichier.txt");
```

## ğŸ” Exemple de performance

Pour un ensemble de fichiers texte :
- DÃ©duplication : ~60-80% d'espace Ã©conomisÃ©
- Compression : ~40-60% de rÃ©duction supplÃ©mentaire
- Temps de traitement : quelques ms par MB
- Reconstruction instantanÃ©e

## âš™ï¸ Configuration requise

- Java 17+
- PostgreSQL 12+
- BibliothÃ¨ques :
  - zstd-jni (compression)
  - rabin-fingerprint (chunking)
  - HikariCP (pool de connexions)
- Espace disque pour /storage/

## ğŸ› ï¸ Maintenance

- Les chunks sont datÃ©s (created_at)
- Le stockage et la base peuvent Ãªtre rÃ©initialisÃ©s
- Monitoring des ratios de compression
- DÃ©tection automatique des fichiers problÃ©matiques